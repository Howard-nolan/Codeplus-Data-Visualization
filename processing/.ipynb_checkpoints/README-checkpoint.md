# Visualization in Python for Duke Data Scientists

# Authors: Alyssa Ting, Susan Feng, Joey Nolan

## Data Cleaning and Processing
<img src="/images/flowchart.png" alt="flowchart" width="500"/>

### Creating synthetic data

As discussed in the general README, linked [here](https://gitlab.oit.duke.edu/at341/codeplus-celine-dcc-package/-/tree/master/README.md), the AST and InfoUSA datasets we used are not available to the public. Therefore, we've created synthetic data formatted in the exact same way as these two datasets so that this repository can run with test data.

**Creating AST Data:** Our researcher allowed us to randomly sample 1% of her AST dataset. We use the random library's ```.sample()``` to randomly pull 1% of the data and saved that in the ```data``` folder in this repository for anyone to access.

**Creating InfoUSA Data:** We generated all random numbers for this dataset. However, we kept the original column names and used plausible value ranges to generate random values for each column. Since the original InfoUSA dataset has information on each household's latitude and longitude coordinates, we needed to randomly generate coordinates within the US. Our first approach to this was by restricting the latitude and longitude so that the points were generally in the area of the US. However, as seen below, this resulted in lots of points being generated outside of the US, which gave an inaccurate perspective on what our visualizations should output. Therefore, we adjusted our approach so that each coordinate generated was accurately restricted to be within the US, which gave us the result in the second image. This, however, takes a significantly longer runtime- because for each coordinate generated, we must check to see if it is within the US. It took around 25 minutes to generate 75,000 points. 

**Note:** It is important to note that the InfoUSA data has around 126 million observations across its thousands of files, and so this synthetic data is just a small example of it. We are also only using 1% of the AST data- so not all the visualizations will appear or 'behave' exactly like the ones shown on the general README, which were made with the original datasets. For example, since there are only around 1,000 tanks in our synthetic data, the distances between each household and the tank nearest to it will be much larger than what we would find using the original data. As you increase the number of points generated in the synthetic data, the data should begin to mimic the original InfoUSA data and the visualizations will become closer to the ones outputted by the original datasets. 

![Example](/images/synthetic_original.png)
![Example](/images/synthetic_fixed.png)

### File Merging: merging thousands of ```.txt``` files and writing the merged file as a ```.parquet``` file
The InfoUSA dataset has household-level census data for 15 years, 2006 to 2020. Our work uses only the 2020 data. The 2020 data alone contains 38,248 ```.txt``` files, each file representing the census data per each household in one zip code. However, in order to analyze and visualize this data meaningfully, we used the ```pyarrow``` engine and the ```.parquet``` file format in order to combine and store all this data in one, merged ```.parquet``` file.  

Since each file had around a few thousand observations each and 54 columns, merging thousands of these files and storing them into a single ```.csv``` file quickly became tough on both processing power and memory usage– although we were able to merge all 38,248 files and write it as a ```.csv``` file, we weren’t able to read in that ```.csv``` file as it took up too much memory. 

To get around this issue, we chose to use the pyarrow engine when reading in each ```.txt``` file, merge it with the other files, and then save the combined files as a ```.parquet``` file. This engine allows for multithreading when reading in files, which allows for a better, more efficient use of the CPU cores, thus resulting in a faster runtime. We also chose to save the complete combined file as a Parquet file, which is optimized for storing and reading large sets of data. As it is column-based as opposed to row-based, like a CSV file, a Parquet file only focuses on the data relevant to the user’s query. In a one-hundred column table, for example, if the user is only using a few of these columns, Parquet files enables the user to fetch only the required columns and values and load those into memory. Parquet files can also be read in and exported using pandas, which is an added bonus when working with pandas dataframes.

However, even using Pyarrow and Parquet, we were unable to combine all 38,248 files in one go– running into memory issues even when using 208GB of RAM, we decided to then part our data into four subsets, combine each of those subsets, filter for only the columns we will use, then merge them to each other for our final, combined file. This file was stored as a Parquet file, and with more than 190 million rows and 10 columns, it takes less than 30 seconds to read in (using 8 CPUs and 32GB RAM). 

### Data processing: filtering, merging and aggregating data
We were provided multiple sources of data, each of which include extensive amounts of information. In each of these datasets, we are only use the few number of columns relevant to our work, and so in order to make our work more readable and memory-efficient, we have multiple processing notebooks in which we filter through our data for select columns, and then save the narrowed-down versions of those datasets. 

In addition, in order to visualize our data in meaningful ways, we merged data from various different sources in order to gain insight on the nuances of the data. For example, we were interested in investigating the exposure of petrochemical tanks to a variety of natural hazards, such as earthquakes and hurricanes. To do so, we classified each tank by the county in which it was found, and merged this dataframe with the National Risk Index data, which provided risk index values for each natural hazard for each county in the United States. Like this, we were able to associate a risk index for various natural disasters to each tank. We then used our Nearest Neighbor Analysis code (to be explained in further detail below) to find the tank nearest to each household, and associate the risk of each tank spilling to a number of households. Like this, we were able to visualize which households and areas in the United States were more vulnerable to tank spillages according to natural hazards. 

Finally, we performed numerous different aggregations of the data in order to get a more generalized view of certain aspects. An example of this is our aggregated ‘ChildrenHHCount’ column in the data, taking it from the number of children per household and summarizing it to find the number of children in each county. Doing so, we were able to provide our researcher a way to identify areas in the US more dense in children to focus her analysis. 

### Nearest Neighbor Analysis: calculating the shortest distance between two points
We used a version of nearest neighbor analysis to investigate the distances between households and the storage tank nearest to them. To do this analysis, we first developed a brute-force algorithm that would take each household, loop over each tank in the AST dataset, calculate the distance between those two points, and keep the shortest distance. This nested for loop method, when working with tens of millions of households and nearly a hundred thousand tanks, quickly proved both inefficient and unfeasible. Therefore, after research, we found code written and published by the University of Helsinki, which finds and calculates the distances between buildings and the nearest transport stop. 

This code uses scikit-learn, an open-source machine learning Python library that provides simple, yet efficient tools for predictive data analysis. From this library, the code published by the University of Helsinki specifically uses the ```BallTree``` method in the library’s neighbors module. This unsupervised learning method finds a predefined number of training samples closest in distance to the new point, and uses this to intelligently pinpoint each point nearest to the point of interest. Using this method, we were able to identify the tank closest to 53 million households in less than an hour. 

However, to ensure the accuracy of the method published by the University of Helsinki, we used both their algorithm and our brute-force algorithm on a small subset of households and tanks. Comparing the results of the two, we found that the University of Helsinki’s algorithm was finding the same closest tanks to each household as our brute-force method, but the distance calculated was off by hundreds of meters in both the positive and negative directions. Using this information, taking a closer look at the University of Helsinki’s code, we identified that their method to calculate distance based on radians was slightly inaccurate as opposed to the one we used in our brute-force algorithm.

Thus, we only used the University of Helsinki’s code to efficiently and accurately identify the latitude and longitude coordinates of the tank nearest to each household, then used these coordinates and the coordinates of the household it is the nearest to in order to calculate the distance between the two points.

Even using this more efficient version of our code, we were only able to find the distances between each household and the tank nearest to it for around 53 million households, as we filtered through all the households for only those with children in them. Running our adapted algorithm on around 190 million points proved time and memory inefficient. Our researcher, Celine, noted that in the future we can use spatial joins to identify households within five miles of a tank, and then perform the distance calculations on this much smaller subset of the data.

### Transformations between coordinate systems: converting from EPSG 4326 to EPSG 3857
As we are working with a variety of spatial data, largely information on the latitude and longitude of certain points like households or tanks, it was necessary for us to always be mindful of the coordinate system or projection our data was in, as well us the system our visualizations library needed it to be in. 

EPSG 4326 refers to a coordinate system within a database of coordinate system information published by the European Petroleum Survey Group (EPSG). It is, specifically, latitude and longitude coordinates on the WGS84 reference ellipsoid, and is the coordinate system our InfoUSA and AST data are in. 

The cuxfilter library, though, uses a world map background provided by OpenStreetMaps, which uses EPSG 3857 (a Spherical Mercator projection coordinate system). Therefore, every time we used the cuxfilter library to plot our points, we used the pyproj interface, which allows us to use the PROJ coordinate transformation software to transform our EPSG 4326 coordinates to EPSG 3857. 

### Spatial joins: using ```.sjoin()``` to find the intersections between geometries
We took advantage of the GeoPandas library’s ```.sjoin()``` function to produce a few of our visualizations. This function performs a spatial join of two GeoDataFrames, and by setting the ‘predicate’ parameter to ‘intersects’, outputs a new GeoDataFrame which only includes the observations with geometries that are the intersections of the two original GeoDataFrames.

This proved useful when classifying the tanks by county, for example, as we were only provided the geometries of each tank. When conducting a spatial join between this GeoDataFrame and another GeoDataFrame including geometries for each county across the US, we were able to identify which tanks belonged to each county. This was done by looping over each county in the counties GeoDataFrame, performing a spatial join with that geometry and the AST dataset to identify which tanks belonged to that county, and adding a column to the corresponding tank in the AST dataset indicating that county. This crucial added information on each tank allowed us to make the risk visualizations as detailed above. In addition, this logic also allowed us to count the number of households within five miles of a tank in each county, providing Celine with a way to identify which counties to focus her research on.